{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODLs0ZqpLwc/JxYLGcKv7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunyux/Exploring-Thompson-Sampling-for-CNN-Hyperparameter-Optimization/blob/main/Exploring_Thompson_Sampling_for_CNN_Hyperparameter_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qO3NNXUq1A9U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms,models\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get_data_loaders\n",
        "\n",
        "*  mnist\n",
        "*  cifar10\n"
      ],
      "metadata": {
        "id": "DpgOg1Jj1KJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(dataset_name, batch_size):\n",
        "    if dataset_name == \"mnist\":\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "        input_channels = 1\n",
        "    elif dataset_name == \"cifar10\":\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "        input_channels = 3\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader, input_channels"
      ],
      "metadata": {
        "id": "1yNVMI1L1a-w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module\n",
        "\n",
        "*  LeNet\n",
        "*  VGG16&VGG19\n",
        "*  Resnet50\n",
        "*  Transpose\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7nPPdudc1dpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_model(model_choice):\n",
        "    if model_choice == \"LeNet\":\n",
        "        return LeNet()\n",
        "    elif model_choice == \"VGG16\":\n",
        "        return get_vgg_model(\"VGG16\", num_classes=10)\n",
        "    elif model_choice == \"VGG19\":\n",
        "        return get_vgg_model(\"VGG19\", num_classes=10)\n",
        "    elif model_choice == \"ResNet50\":\n",
        "        return get_resnet50_model(num_classes=10)\n",
        "    elif model_choice == \"Transformer\":\n",
        "        return TransformerModel(num_classes=10)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_choice. Choose from 'LeNet', 'VGG16', 'VGG19', or 'ResNet50'.\")"
      ],
      "metadata": {
        "id": "Kr_FRLBL19c0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self, input_channels=1, num_classes=10, input_size=28):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DzfWQJw41kQ5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vgg_model(model_name, num_classes):\n",
        "    if model_name == \"VGG16\":\n",
        "        model = models.vgg16(pretrained=True)\n",
        "    elif model_name == \"VGG19\":\n",
        "        model = models.vgg19(pretrained=True)\n",
        "\n",
        "    # Freeze all convolutional layers to fine-tune only the classifier\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Modify the classifier for the specific number of classes\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "TqcqPXXg1mcY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet50_model(num_classes):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "\n",
        "    # Freeze all layers except the last fully connected layer\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the final fully connected layer to match the number of classes\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "VN59Lp2X1zH0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transpose(nn.Module):\n",
        "    def __init__(self, dim0, dim1):\n",
        "        super().__init__()\n",
        "        self.dim0 = dim0\n",
        "        self.dim1 = dim1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.transpose(self.dim0, self.dim1)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3, patch_size=4, hidden_dim=256, num_heads=8, num_layers=6, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Calculate number of patches (assuming 32x32 input for CIFAR-10)\n",
        "        self.num_patches = (32 // patch_size) ** 2\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, hidden_dim, kernel_size=patch_size, stride=patch_size),\n",
        "            nn.Flatten(2),\n",
        "            Transpose(1, 2)\n",
        "        )\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, hidden_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.head = nn.Linear(hidden_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.transformer(x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.norm(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.zeros_(m.bias)\n",
        "            nn.init.ones_(m.weight)"
      ],
      "metadata": {
        "id": "l4XmFA4p2DGd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thompson Sampling\n",
        "\n",
        "*   LinearThompsonSampler\n",
        "*   NonLinearThompsonSampler(Zhang, W., Zhou, D., Li, L., & Gu, Q. (2020). Neural Thompson Sampling. Proceedings of the International Conference on Learning Representations.)\n",
        "\n"
      ],
      "metadata": {
        "id": "QVOj58I92bcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ThompsonSampling1(input_dim):\n",
        "    class LinearThompsonSampler:\n",
        "        def __init__(self, input_dim):\n",
        "            self.A = np.identity(input_dim)\n",
        "            self.b = np.zeros(input_dim)\n",
        "\n",
        "        def sample(self):\n",
        "            A_inv = np.linalg.inv(self.A)\n",
        "            theta_hat = np.dot(A_inv, self.b)\n",
        "            theta_sampled = np.random.multivariate_normal(theta_hat, A_inv)\n",
        "            return theta_sampled\n",
        "\n",
        "        def update(self, x_t, reward):\n",
        "            x_t = np.reshape(x_t, (-1, 1))\n",
        "            self.A += np.dot(x_t, x_t.T)\n",
        "            self.b += reward * x_t.flatten()\n",
        "\n",
        "    return LinearThompsonSampler(input_dim)"
      ],
      "metadata": {
        "id": "sVFbpEYA2WbF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ThompsonSampling(input_dim):\n",
        "    class NonLinearThompsonSampler:\n",
        "        def __init__(self, input_dim):\n",
        "            # Neural network for mean approximation\n",
        "            self.network = nn.Sequential(\n",
        "                nn.Linear(input_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, 1)\n",
        "            )\n",
        "\n",
        "            self.memory_x = []\n",
        "            self.memory_y = []\n",
        "            self.memory_size = 1000\n",
        "            self.batch_size = 32\n",
        "            self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n",
        "            self.beta = 1.0\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            self.network.to(self.device)\n",
        "\n",
        "        def sample(self):\n",
        "            \"\"\"\n",
        "            Sample from the posterior distribution using Thompson Sampling\n",
        "            Returns:\n",
        "                numpy.ndarray: Sampled hyperparameters\n",
        "            \"\"\"\n",
        "            sample_points = torch.randn(100, input_dim).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                predictions = []\n",
        "                for _ in range(10):  # Monte Carlo sampling\n",
        "                    pred = self.network(sample_points)\n",
        "                    predictions.append(pred)\n",
        "\n",
        "                mean_pred = torch.mean(torch.stack(predictions), dim=0)\n",
        "                std_pred = torch.std(torch.stack(predictions), dim=0)\n",
        "\n",
        "                # Thompson sampling with uncertainty\n",
        "                sampled_values = mean_pred + self.beta * std_pred\n",
        "                best_idx = torch.argmax(sampled_values)\n",
        "\n",
        "            return sample_points[best_idx].cpu().numpy()\n",
        "\n",
        "        def update(self, x_t, reward):\n",
        "            \"\"\"\n",
        "            Update the model with new observation\n",
        "            Args:\n",
        "                x_t (numpy.ndarray): Input hyperparameters\n",
        "                reward (float): Observed reward (accuracy)\n",
        "            \"\"\"\n",
        "            x_t = torch.FloatTensor(x_t).to(self.device)\n",
        "            reward = torch.FloatTensor([reward]).to(self.device)\n",
        "            self.memory_x.append(x_t)\n",
        "            self.memory_y.append(reward)\n",
        "\n",
        "            # Keep memory within size limit\n",
        "            if len(self.memory_x) > self.memory_size:\n",
        "                self.memory_x.pop(0)\n",
        "                self.memory_y.pop(0)\n",
        "\n",
        "            # Train network if enough samples\n",
        "            if len(self.memory_x) >= self.batch_size:\n",
        "                # Sample batch\n",
        "                indices = np.random.choice(len(self.memory_x), self.batch_size, replace=False)\n",
        "                batch_x = torch.stack([self.memory_x[i] for i in indices])\n",
        "                batch_y = torch.stack([self.memory_y[i] for i in indices])\n",
        "\n",
        "                # Train step\n",
        "                self.optimizer.zero_grad()\n",
        "                pred = self.network(batch_x)\n",
        "                loss = nn.MSELoss()(pred, batch_y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Adjust exploration parameter\n",
        "                self.beta = max(0.1, self.beta * 0.995)\n",
        "\n",
        "    return NonLinearThompsonSampler(input_dim)"
      ],
      "metadata": {
        "id": "v_OLij3V2r7q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train data & Validate"
      ],
      "metadata": {
        "id": "GNx0pWDY4RXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, config, device):\n",
        "    model = model.to(device)\n",
        "    optimizer = getattr(optim, config[\"optimizer\"])(model.parameters(), lr=config[\"learning_rate\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(3):\n",
        "        print(f\"\\nEpoch {epoch}\")\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        accuracy1 = validate_model(model, train_loader, criterion, device)\n",
        "        print(f\"\\naccuracy1 {accuracy1}\")\n",
        "    accuracy = validate_model(model, train_loader, criterion, device)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "lY-Hilpu4L76"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "iQKtI7DM4ZRX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main function for find best performing hyperparameter"
      ],
      "metadata": {
        "id": "_l-gYQit4kam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(model_choice, num_iterations=100):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Initialize Thompson Sampling and Random Search\n",
        "    sampler = ThompsonSampling(input_dim=3)\n",
        "    random_search_results = []\n",
        "    thompson_results = []\n",
        "    accuracies_random_search = []\n",
        "    accuracies_thompson = []\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        print(f\"Testing Iteration {iteration}\", flush=True)\n",
        "\n",
        "        # --- Thompson Sampling ---\n",
        "        x_t = np.random.rand(3)\n",
        "        theta_sampled = sampler.sample()\n",
        "\n",
        "        learning_rate = 1e-5 + (abs(theta_sampled[0] % 1) * (1e-3 - 1e-5))\n",
        "        batch_size = int(abs(theta_sampled[1] * 64)) % 128 + 32\n",
        "        optimizer = random.choice([\"SGD\", \"Adam\"])\n",
        "\n",
        "        thompson_config = {\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"optimizer\": optimizer,\n",
        "        }\n",
        "        train_loader, _, _ = get_data_loaders(\"cifar10\", batch_size=thompson_config[\"batch_size\"])\n",
        "        model = select_model(model_choice)\n",
        "        accuracy_thompson = train_model(model, train_loader, thompson_config, device)\n",
        "        accuracies_thompson.append(accuracy_thompson)\n",
        "        sampler.update(x_t, accuracy_thompson)\n",
        "        thompson_results.append((thompson_config, accuracy_thompson))\n",
        "        print(f\"Thompson Sampling Iteration {iteration}: Accuracy = {accuracy_thompson:.4f}, Config = {thompson_config}\")\n",
        "\n",
        "        # --- Random Search ---\n",
        "        learning_rate_rand = random.uniform(1e-5, 1e-3)\n",
        "        batch_size_rand = random.choice([32, 64, 128])\n",
        "        optimizer_rand = random.choice([\"SGD\", \"Adam\"])\n",
        "\n",
        "        random_search_config = {\n",
        "            \"learning_rate\": learning_rate_rand,\n",
        "            \"batch_size\": batch_size_rand,\n",
        "            \"optimizer\": optimizer_rand,\n",
        "        }\n",
        "        train_loader, _, _ = get_data_loaders(\"cifar10\", batch_size=random_search_config[\"batch_size\"])\n",
        "        model = select_model(model_choice)\n",
        "        accuracy_random_search = train_model(model, train_loader, random_search_config, device)\n",
        "        accuracies_random_search.append(accuracy_random_search)\n",
        "        random_search_results.append((random_search_config, accuracy_random_search))\n",
        "        print(f\"Random Search Iteration {iteration}: Accuracy = {accuracy_random_search:.4f}, Config = {random_search_config}\")\n",
        "\n",
        "    best_config_thompson, best_accuracy_thompson = max(thompson_results, key=lambda x: x[1])\n",
        "    best_config_random_search, best_accuracy_random_search = max(random_search_results, key=lambda x: x[1])\n",
        "\n",
        "    print(f\"\\nBest Thompson Sampling Config: {best_config_thompson}, Best Accuracy: {best_accuracy_thompson:.4f}\")\n",
        "    print(f\"Best Random Search Config: {best_config_random_search}, Best Accuracy: {best_accuracy_random_search:.4f}\")\n",
        "\n",
        "    # Plot accuracy comparison over iterations\n",
        "    plt.plot(range(len(accuracies_thompson)), accuracies_thompson, label='Thompson Sampling', color='b')\n",
        "    plt.plot(range(len(accuracies_random_search)), accuracies_random_search, label='Random Search', color='r')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Thompson Sampling vs Random Search Accuracy Trend')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_choices = [\"ResNet50\"]\n",
        "    for model_choice in model_choices:\n",
        "        main(model_choice)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0x9xNha4j8l",
        "outputId": "6c4e2df8-e0e9-468b-a59f-dadc933aa54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Iteration 0\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 78.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 124MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main function for test best performing hyperparameter\n",
        "\n",
        "\n",
        "* Change train_model epoch to a larger one\n",
        "* Put hyperparameter in configurations list(eg:(\"ResNet50\", 6.238483e-05, 66, \"Adam\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "O6kvYQsR5oav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    configurations = [\n",
        "        # put data here !!!!!!!!!\n",
        "         (\"ResNet50\", 6.238483e-05, 66, \"Adam\")\n",
        "\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    best_configs = {}\n",
        "    for model_name, learning_rate, batch_size, optimizer in configurations:\n",
        "        print(f\"\\nTesting: {model_name}\")\n",
        "        train_loader, test_loader, input_channels = get_data_loaders(\"cifar10\", batch_size=batch_size)\n",
        "        model = select_model(model_name)\n",
        "        config = {\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"optimizer\": optimizer\n",
        "        }\n",
        "\n",
        "        accuracy = train_model(model, train_loader, config, device)\n",
        "        result = {\n",
        "            \"Model\": model_name,\n",
        "            \"Learning Rate\": learning_rate,\n",
        "            \"Batch Size\": batch_size,\n",
        "            \"Optimizer\": optimizer,\n",
        "            \"Accuracy\": accuracy\n",
        "        }\n",
        "        results.append(result)\n",
        "        if model_name not in best_configs or accuracy > best_configs[model_name]['Accuracy']:\n",
        "            best_configs[model_name] = result\n",
        "\n",
        "        print(f\"Final Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\nBest Configurations per Model Type:\")\n",
        "    for model_name, best_config in best_configs.items():\n",
        "        print(f\"\\nBest {model_name} Configuration:\")\n",
        "        print(f\"Learning Rate: {best_config['Learning Rate']:.6e}\")\n",
        "        print(f\"Batch Size: {best_config['Batch Size']}\")\n",
        "        print(f\"Optimizer: {best_config['Optimizer']}\")\n",
        "        print(f\"Accuracy: {best_config['Accuracy']:.4f}\")\n",
        "\n",
        "    print(\"\\nFinal Detailed Results:\")\n",
        "    for result in results:\n",
        "        print(f\"{result['Model']}\\tLR: {result['Learning Rate']:.6e}\\tBatch: {result['Batch Size']}\\t\"\n",
        "              f\"Optimizer: {result['Optimizer']}\\tAccuracy: {result['Accuracy']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ],
      "metadata": {
        "id": "iUVEC3Kk4bxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}